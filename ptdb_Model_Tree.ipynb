{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "from tslearn.clustering import KShape\n",
    "import numpy as np\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "Restriction = '3-SD'\n",
    "numberOfFeatures2 = numberOfWindows = 187\n",
    "ep = 200\n",
    "ep_es = 35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureSetB1 = pd.read_csv('Windows/ptdb_187Ws.csv')\n",
    "FeatureSetB1.drop(\"Unnamed: 0\",axis=1,inplace = True)\n",
    "FeatureSetB1 = FeatureSetB1.drop('index', axis=1)\n",
    "tree = tree.DecisionTreeClassifier()\n",
    "gbc = GradientBoostingClassifier(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "FeatureSetB2 = FeatureSetB1.sample(frac=1)\n",
    "FeatureSetB3 = FeatureSetB2.iloc[:, :-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "B_class_0 = FeatureSetB3[FeatureSetB2['Class'] == 0]\n",
    "B_class_1 = FeatureSetB3[FeatureSetB2['Class'] == 1]\n",
    "\n",
    "B_class_F0 = FeatureSetB2[FeatureSetB2['Class'] == 0]\n",
    "B_class_F1 = FeatureSetB2[FeatureSetB2['Class'] == 1]\n",
    "\n",
    "\n",
    "# Calculate the Z-scores for each feature\n",
    "z_score_0 = (B_class_0 - B_class_0.mean()) / B_class_0.std()\n",
    "z_score_1 = (B_class_1 - B_class_1.mean()) / B_class_1.std()\n",
    "\n",
    "# Set a threshold for the Z-score\n",
    "threshold_0 = 2.5\n",
    "threshold_1 = 3\n",
    "\n",
    "# Identify the outliers based on the threshold\n",
    "outliers_0 = np.abs(z_score_0) > threshold_0\n",
    "outliers_1 = np.abs(z_score_1) > threshold_1\n",
    "\n",
    "# Remove the outliers from the dataset\n",
    "B_class_F0 = B_class_F0[~outliers_0.any(axis=1)]\n",
    "B_class_F1 = B_class_F1[~outliers_1.any(axis=1)]\n",
    "\n",
    "# Split the dataset into two classes and perform clustering as described in the previous answer\n",
    "merged_df = pd.concat([B_class_F1, B_class_F0])\n",
    "merged_df = merged_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = merged_df.iloc[:, :-2]\n",
    "y = merged_df[['Class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ros = RandomOverSampler(random_state=0,sampling_strategy=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, X_test, y_train1, y_test = train_test_split(X,y, test_size=0.2, random_state=1, stratify=y, shuffle=True)\n",
    "\n",
    "x,y = ros.fit_resample(X_train1,y_train1)\n",
    "\n",
    "X_train2, X_val, y_train2, y_val = train_test_split(x,y, test_size=0.2, random_state=1, stratify=y, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rye - Artificial Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout,Flatten, Activation\n",
    "from sklearn import metrics\n",
    "from keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "mod=sys.modules[__name__]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import tree\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import class_weight\n",
    "tree = tree.DecisionTreeClassifier()\n",
    "\n",
    "acc_score = []\n",
    "re_score = []\n",
    "pre_score = []\n",
    "history_1 = []\n",
    "f_score = []\n",
    "auroc_score = []\n",
    "auprc_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.Normalizer().fit(X_train2)\n",
    "\n",
    "X_train4 = scaler.transform(X_train2)\n",
    "X_test4 = scaler.transform(X_test)\n",
    "X_val4 = scaler.transform(X_val)\n",
    "\n",
    "#y_train2 = to_categorical(y_train2)\n",
    "#y_test = to_categorical(y_test)\n",
    "#y_val = to_categorical(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(max_depth=None,bootstrap=True,min_samples_leaf=2,min_samples_split=2,max_features='sqrt', random_state=42,n_estimators=1800)\n",
    "\n",
    "#scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "#scaler = preprocessing.RobustScaler().fit(X_train)\n",
    "#scaler =  preprocessing.MinMaxScaler().fit(X_train)\n",
    "scaler = preprocessing.Normalizer().fit(X_train4)\n",
    "\n",
    "X_train4 = scaler.transform(X_train4)\n",
    "X_test4 = scaler.transform(X_test4)\n",
    "X_val4 = scaler.transform(X_val4)\n",
    "\n",
    "history = model.fit(X_train4,y_train2)#, epochs = ep,batch_size=512,validation_data=(X_val4, y_val), callbacks=[stop_early],shuffle=True)#,use_multiprocessing=True\n",
    "history_1.append(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "y_pred_classes = model.predict(X_test4)\n",
    "y_test_classes = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate F1 score macro and Precision macro\n",
    "f1_value = f1_score(y_test_classes, y_pred_classes, average='macro')\n",
    "precision = precision_score(y_test_classes, y_pred_classes, average='macro')\n",
    "sensitivity = recall_score(y_test_classes, y_pred_classes,average='macro')#,pos_label = 1, average='binary')\n",
    "accuracy = accuracy_score(y_test_classes, y_pred_classes)\n",
    "\n",
    "acc_score.append(accuracy)\n",
    "re_score.append(sensitivity)\n",
    "pre_score.append(precision)\n",
    "f_score.append(f1_value)\n",
    "\n",
    "k = 1 \n",
    "\n",
    "avg_acc_score = sum(acc_score)/k\n",
    "avg_recall_score = sum(re_score)/k\n",
    "avg_precision_score = sum(pre_score)/k\n",
    "avg_f1_score = sum(f_score)/k\n",
    "\n",
    "sensitivity = avg_recall_score\n",
    "precision = avg_precision_score\n",
    "accuracy = avg_acc_score\n",
    "f1_score = avg_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg accuracy : 0.9922360248447205\n",
      "Avg Reccall : 0.7484326018808778\n",
      "Avg Precision : 0.7976525821596244\n",
      "Avg F1_score : 0.7707695593365131\n"
     ]
    }
   ],
   "source": [
    "#print('accuracy of each fold - {}'.format(acc_score))\n",
    "print('Avg accuracy : {}'.format(avg_acc_score))\n",
    "#print('Recall of each fold - {}'.format(re_score))\n",
    "print('Avg Reccall : {}'.format(avg_recall_score))\n",
    "#print('Precision of each fold - {}'.format(pre_score))\n",
    "print('Avg Precision : {}'.format(avg_precision_score))\n",
    "#print('F1_Score of each fold - {}'.format(f_score))\n",
    "print('Avg F1_score : {}'.format(avg_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "# #scaler = preprocessing.RobustScaler().fit(X_train)\n",
    "# #scaler =  preprocessing.MinMaxScaler().fit(X_train)\n",
    "# scaler = preprocessing.Normalizer().fit(x)\n",
    "\n",
    "# x = scaler.transform(x)\n",
    "# y = to_categorical(y)\n",
    "\n",
    "\n",
    "# precision = cross_val_score(tree,x,y,cv=5,scoring='precision_macro')#,average='macro')\n",
    "# recall = cross_val_score(tree,x,y,cv=5,scoring='recall_macro')#,average='macro')\n",
    "# accuracy = cross_val_score(tree,x,y,cv=5,scoring='accuracy')#,average='macro')\n",
    "# f1 = cross_val_score(tree,x,y,cv=5,scoring='f1_macro')#,average='macro')\n",
    "\n",
    "# sensitivity = np.mean(recall)\n",
    "# precision = np.mean(precision)\n",
    "# accuracy = np.mean(accuracy)\n",
    "# f1_score = np.mean(f1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "\n",
    "csv_columns = ['model-type','precision','sensitivity','f1-score','accuracy','NumberOfWindows','Epochs','Run_Time']\n",
    "dict_data = [{'model-type':'RF', 'precision': precision,'sensitivity': sensitivity,'f1-score': f1_score,'accuracy': accuracy,'NumberOfWindows':numberOfWindows,\"Epochs\":ep}]\n",
    "metric_file = \"Results_Apr3/ptdb_RF_1.csv\"\n",
    "\n",
    "file_exists = os.path.isfile(metric_file)\n",
    "try:\n",
    "    with open(metric_file, 'a') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        for data in dict_data:\n",
    "            writer.writerow(data)\n",
    "except IOError:\n",
    "    print(\"I/O error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f3ba9e726005c067fbc0eaab20e31f21cd7f7e132fec231b7dd4c5a2981fed5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
